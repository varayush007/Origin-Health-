{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3J3wE9C1fRQu",
        "outputId": "20fdae32-d420-4965-e4fe-0d3bd4c1c5a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "['006_HC.png', '003_HC.png', '010_2HC.png', '002_HC.png', '011_HC.png', '009_HC.png', '010_HC.png', '005_HC.png', '007_HC.png', '004_HC.png', '008_HC.png', '000_HC.png', '001_HC.png', '013_HC.png', '014_3HC.png', '019_HC.png', '014_2HC.png', '014_HC.png', '012_HC.png', '017_2HC.png', '016_HC.png', '018_HC.png', '019_2HC.png', '015_HC.png', '017_HC.png', '025_HC.png', '026_2HC.png', '020_HC.png', '023_HC.png', '026_HC.png', '021_HC.png', '023_2HC.png', '027_HC.png', '022_2HC.png', '022_HC.png', '028_HC.png', '024_HC.png', '030_HC.png', '033_HC.png', '037_HC.png', '036_HC.png', '035_HC.png', '032_HC.png', '032_2HC.png', '029_HC.png', '034_HC.png', '038_HC.png', '031_HC.png', '039_HC.png', '033_2HC.png', '043_HC.png', '042_HC.png', '040_HC.png', '047_HC.png', '046_HC.png', '045_HC.png', '041_HC.png', '044_HC.png', '049_HC.png', '048_HC.png', '056_HC.png', '054_HC.png', '053_HC.png', '051_HC.png', '050_2HC.png', '052_HC.png', '060_HC.png', '050_HC.png', '055_HC.png', '057_HC.png', '058_HC.png', '059_HC.png', '064_2HC.png', '063_2HC.png', '066_HC.png', '063_HC.png', '065_HC.png', '061_2HC.png', '063_3HC.png', '062_HC.png', '067_HC.png', '064_HC.png', '066_2HC.png', '068_2HC.png', '061_HC.png', '073_HC.png', '078_HC.png', '077_HC.png', '075_HC.png', '068_HC.png', '072_HC.png', '071_HC.png', '070_HC.png', '074_HC.png', '076_HC.png', '069_HC.png', '082_2HC.png', '083_HC.png', '084_HC.png', '083_2HC.png', '080_HC.png', '081_HC.png', '085_HC.png', '082_HC.png', '079_HC.png', '087_HC.png', '088_2HC.png', '086_HC.png', '093_2HC.png', '089_HC.png', '094_HC.png', '088_HC.png', '095_HC.png', '093_HC.png', '099_HC.png', '096_HC.png', '098_HC.png', '090_HC.png', '097_HC.png', '092_HC.png', '091_HC.png', '100_2HC.png', '104_HC.png', '107_HC.png', '105_HC.png', '103_HC.png', '108_2HC.png', '100_HC.png', '102_2HC.png', '106_HC.png', '102_HC.png', '101_HC.png', '107_2HC.png', '112_HC.png', '114_HC.png', '108_3HC.png', '115_HC.png', '109_HC.png', '118_HC.png', '113_HC.png', '111_HC.png', '108_HC.png', '117_HC.png', '110_HC.png', '116_HC.png', '121_2HC.png', '125_2HC.png', '123_HC.png', '124_HC.png', '126_2HC.png', '122_HC.png', '125_HC.png', '119_HC.png', '124_2HC.png', '125_3HC.png', '121_HC.png', '120_HC.png', '126_3HC.png', '133_3HC.png', '131_HC.png', '127_HC.png', '132_HC.png', '130_HC.png', '133_2HC.png', '129_HC.png', '133_4HC.png', '128_HC.png', '133_HC.png', '126_HC.png', '136_2HC.png', '136_HC.png', '138_HC.png', '142_HC.png', '134_HC.png', '143_2HC.png', '141_HC.png', '135_HC.png', '140_HC.png', '139_HC.png', '141_2HC.png', '137_HC.png', '138_2HC.png', '152_HC.png', '145_HC.png', '151_HC.png', '146_2HC.png', '143_HC.png', '150_HC.png', '149_HC.png', '144_2HC.png', '146_HC.png', '153_HC.png', '147_HC.png', '144_HC.png', '148_HC.png', '158_HC.png', '157_HC.png', '162_HC.png', '156_HC.png', '160_HC.png', '159_HC.png', '163_HC.png', '161_HC.png', '156_2HC.png', '154_HC.png', '155_HC.png', '164_2HC.png', '165_HC.png', '171_HC.png', '167_HC.png', '173_HC.png', '168_HC.png', '170_HC.png', '172_HC.png', '166_HC.png', '164_HC.png', '174_HC.png', '169_HC.png', '181_HC.png', '179_HC.png', '175_HC.png', '182_HC.png', '178_2HC.png', '178_HC.png', '185_HC.png', '176_HC.png', '180_HC.png', '177_HC.png', '186_HC.png', '184_HC.png', '183_HC.png', '192_HC.png', '194_HC.png', '188_HC.png', '187_HC.png', '193_2HC.png', '190_HC.png', '193_HC.png', '191_HC.png', '189_HC.png', '192_2HC.png', '196_HC.png', '203_HC.png', '198_HC.png', '202_HC.png', '198_2HC.png', '195_HC.png', '203_4HC.png', '203_2HC.png', '195_2HC.png', '199_HC.png', '200_HC.png', '203_3HC.png', '197_HC.png', '201_HC.png', '204_HC.png', '213_HC.png', '210_2HC.png', '205_HC.png', '214_HC.png', '206_HC.png', '211_HC.png', '212_HC.png', '210_HC.png', '213_2HC.png', '209_HC.png', '208_HC.png', '207_HC.png', '221_HC.png', '215_HC.png', '220_HC.png', '223_2HC.png', '220_2HC.png', '222_HC.png', '218_HC.png', '217_HC.png', '219_HC.png', '216_HC.png', '223_HC.png', '227_HC.png', '230_HC.png', '229_HC.png', '229_2HC.png', '227_2HC.png', '228_HC.png', '226_HC.png', '225_2HC.png', '225_HC.png', '230_2HC.png', '224_HC.png', '235_2HC.png', '236_HC.png', '241_HC.png', '233_HC.png', '235_HC.png', '237_2HC.png', '237_HC.png', '240_HC.png', '232_HC.png', '238_HC.png', '242_HC.png', '231_HC.png', '234_HC.png', '237_3HC.png', '239_HC.png', '243_3HC.png', '246_HC.png', '245_HC.png', '248_HC.png', '244_HC.png', '245_2HC.png', '243_HC.png', '249_HC.png', '243_2HC.png', '245_3HC.png', '247_HC.png', '259_HC.png', '253_HC.png', '256_HC.png', '258_HC.png', '251_HC.png', '261_HC.png', '252_HC.png', '254_HC.png', '255_HC.png', '257_HC.png', '260_HC.png', '250_HC.png', '260_2HC.png', '271_HC.png', '270_HC.png', '267_HC.png', '263_HC.png', '262_HC.png', '264_HC.png', '269_HC.png', '265_HC.png', '268_HC.png', '271_2HC.png', '272_HC.png', '266_HC.png', '281_HC.png', '283_HC.png', '278_2HC.png', '273_HC.png', '282_HC.png', '276_HC.png', '274_HC.png', '276_2HC.png', '275_HC.png', '278_HC.png', '277_HC.png', '279_HC.png', '280_HC.png', '284_HC.png', '289_HC.png', '291_2HC.png', '287_HC.png', '294_2HC.png', '291_HC.png', '293_HC.png', '288_HC.png', '285_HC.png', '292_HC.png', '286_HC.png', '290_HC.png', '300_2HC.png', '299_2HC.png', '301_HC.png', '295_HC.png', '299_HC.png', '302_HC.png', '294_HC.png', '298_HC.png', '296_HC.png', '297_HC.png', '300_HC.png', '309_HC.png', '308_HC.png', '313_HC.png', '303_HC.png', '306_HC.png', '312_HC.png', '310_HC.png', '307_HC.png', '311_HC.png', '304_HC.png', '312_2HC.png', '305_HC.png', '319_HC.png', '316_HC.png', '315_HC.png', '321_HC.png', '323_2HC.png', '322_HC.png', '320_HC.png', '318_HC.png', '317_HC.png', '314_HC.png', '315_2HC.png', '323_HC.png', '328_HC.png', '327_HC.png', '324_HC.png', '332_HC.png', '326_HC.png', '333_HC.png', '325_HC.png', '327_2HC.png', '331_HC.png', '334_HC.png', '329_HC.png', '330_HC.png', '335_HC.png', '341_HC.png', '345_HC.png', '336_HC.png', '344_HC.png', '340_2HC.png', '337_HC.png', '346_HC.png', '345_2HC.png', '338_HC.png', '343_HC.png', '339_HC.png', '340_HC.png', '342_HC.png', '356_HC.png', '355_HC.png', '357_HC.png', '349_HC.png', '350_HC.png', '351_HC.png', '354_HC.png', '347_HC.png', '352_HC.png', '358_HC.png', '352_2HC.png', '348_HC.png', '353_HC.png', '364_HC.png', '359_HC.png', '359_2HC.png', '361_HC.png', '366_HC.png', '360_2HC.png', '365_HC.png', '362_HC.png', '360_HC.png', '364_2HC.png', '363_HC.png', '367_HC.png', '361_2HC.png', '373_2HC.png', '369_HC.png', '371_HC.png', '370_HC.png', '374_HC.png', '375_HC.png', '377_HC.png', '376_2HC.png', '372_HC.png', '373_HC.png', '368_HC.png', '376_HC.png', '381_HC.png', '384_HC.png', '380_HC.png', '385_HC.png', '379_HC.png', '386_HC.png', '378_HC.png', '383_HC.png', '382_HC.png', '384_2HC.png', '382_2HC.png', '387_HC.png', '392_HC.png', '393_HC.png', '396_HC.png', '392_2HC.png', '394_2HC.png', '388_HC.png', '389_HC.png', '391_HC.png', '397_HC.png', '392_3HC.png', '394_HC.png', '390_HC.png', '395_HC.png', '404_2HC.png', '403_HC.png', '403_2HC.png', '399_HC.png', '399_2HC.png', '400_HC.png', '402_HC.png', '398_HC.png', '405_HC.png', '401_HC.png', '406_HC.png', '404_HC.png', '412_HC.png', '416_HC.png', '413_HC.png', '418_HC.png', '408_HC.png', '411_HC.png', '407_HC.png', '415_HC.png', '410_HC.png', '417_HC.png', '410_2HC.png', '414_HC.png', '409_HC.png', '421_HC.png', '423_HC.png', '426_2HC.png', '422_HC.png', '424_HC.png', '425_HC.png', '426_4HC.png', '425_2HC.png', '419_HC.png', '426_3HC.png', '420_HC.png', '434_HC.png', '426_HC.png', '429_HC.png', '428_HC.png', '430_HC.png', '435_HC.png', '427_HC.png', '431_HC.png', '433_HC.png', '434_2HC.png', '432_HC.png', '431_2HC.png', '436_HC.png', '438_HC.png', '444_HC.png', '440_HC.png', '439_HC.png', '442_HC.png', '441_HC.png', '440_2HC.png', '443_HC.png', '437_2HC.png', '437_HC.png', '444_2HC.png', '452_HC.png', '447_HC.png', '454_HC.png', '449_HC.png', '453_2HC.png', '451_HC.png', '450_HC.png', '453_HC.png', '446_HC.png', '448_HC.png', '445_HC.png', '456_HC.png', '458_HC.png', '463_HC.png', '464_HC.png', '461_HC.png', '462_HC.png', '455_HC.png', '460_HC.png', '462_2HC.png', '457_HC.png', '459_HC.png', '457_2HC.png', '469_HC.png', '471_HC.png', '471_2HC.png', '468_HC.png', '470_HC.png', '465_HC.png', '466_HC.png', '469_2HC.png', '467_HC.png', '472_HC.png', '468_2HC.png', '480_HC.png', '479_HC.png', '478_HC.png', '482_2HC.png', '474_HC.png', '476_HC.png', '482_3HC.png', '477_HC.png', '481_HC.png', '479_2HC.png', '473_HC.png', '475_HC.png', '488_HC.png', '487_HC.png', '485_HC.png', '489_HC.png', '482_HC.png', '490_2HC.png', '486_HC.png', '483_2HC.png', '486_2HC.png', '490_HC.png', '484_HC.png', '483_HC.png', '492_HC.png', '491_HC.png', '495_2HC.png', '498_HC.png', '495_3HC.png', '497_HC.png', '494_HC.png', '495_4HC.png', '496_HC.png', '493_HC.png', '495_HC.png', '500_HC.png', '499_2HC.png', '499_HC.png']\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "folder_path = '/content/drive/My Drive/images'\n",
        "import os\n",
        "print(os.listdir(folder_path))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "# Define the dataset class\n",
        "class LandmarkDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, transform=None, target_size=(256, 256)):\n",
        "        self.landmarks_frame = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.target_size = target_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.landmarks_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img_name = os.path.join(self.root_dir, self.landmarks_frame.iloc[idx, 0])\n",
        "        image = Image.open(img_name).convert('L')\n",
        "        image = image.resize(self.target_size)\n",
        "\n",
        "        landmarks = self.landmarks_frame.iloc[idx, 1:].values.astype('float').reshape(-1, 2)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return {'image': image, 'landmarks': landmarks}\n",
        "\n",
        "# Defining the ResNet-based landmark detection model\n",
        "class LandmarkDetectionModel(nn.Module):\n",
        "    def __init__(self, num_classes=8):\n",
        "        super(LandmarkDetectionModel, self).__init__()\n",
        "        resnet = models.resnet18(pretrained=True)\n",
        "        # Remove the last layer (the fully connected layer)\n",
        "        self.resnet_features = nn.Sequential(*list(resnet.children())[:-1])\n",
        "        # Add custom fully connected layers for landmark detection\n",
        "        self.fc1 = nn.Linear(resnet.fc.in_features, 100)\n",
        "        self.fc2 = nn.Linear(100, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.resnet_features(x)\n",
        "        features = features.view(features.size(0), -1)  # Flatten the feature map\n",
        "        x = torch.relu(self.fc1(features))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Paths to CSV file and image directory\n",
        "csv_file = '/content/drive/MyDrive/role_challenge_dataset_ground_truth.csv'\n",
        "root_dir = '/content/drive/MyDrive/images'\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "\n",
        "landmark_dataset = LandmarkDataset(csv_file=csv_file, root_dir=root_dir, transform=transform)\n",
        "\n",
        "\n",
        "model = LandmarkDetectionModel()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UXdXmjRgmXm",
        "outputId": "504acc28-62d4-4f5b-9962-51e1e5eea5a2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "-qIeFiqjhDru"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset into train, validation, and test sets\n",
        "train_size = int(0.6 * len(landmark_dataset))\n",
        "val_size = int(0.2 * len(landmark_dataset))\n",
        "test_size = len(landmark_dataset) - train_size - val_size\n",
        "train_dataset, val_dataset, test_dataset = random_split(landmark_dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        images, landmarks = batch['image'], batch['landmarks']\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        landmarks = landmarks.view(-1, 8)\n",
        "        loss = criterion(outputs, landmarks.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], Batch [{batch_idx + 1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {epoch_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvYIIc3Wg5u_",
        "outputId": "709c41a1-e0c1-4cf6-acb8-beb2ac6b3793"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Batch [1/3], Loss: 137061.6562\n",
            "Epoch [1/10], Batch [2/3], Loss: 144366.7969\n",
            "Epoch [1/10], Batch [3/3], Loss: 140159.8906\n",
            "Epoch [1/10], Training Loss: 140540.3464\n",
            "Epoch [2/10], Batch [1/3], Loss: 138339.4688\n",
            "Epoch [2/10], Batch [2/3], Loss: 139225.9219\n",
            "Epoch [2/10], Batch [3/3], Loss: 140606.2188\n",
            "Epoch [2/10], Training Loss: 139354.6852\n",
            "Epoch [3/10], Batch [1/3], Loss: 139337.5938\n",
            "Epoch [3/10], Batch [2/3], Loss: 137496.8438\n",
            "Epoch [3/10], Batch [3/3], Loss: 136723.6406\n",
            "Epoch [3/10], Training Loss: 137885.9892\n",
            "Epoch [4/10], Batch [1/3], Loss: 138593.2500\n",
            "Epoch [4/10], Batch [2/3], Loss: 136944.8750\n",
            "Epoch [4/10], Batch [3/3], Loss: 132001.4219\n",
            "Epoch [4/10], Training Loss: 135959.9098\n",
            "Epoch [5/10], Batch [1/3], Loss: 135786.2812\n",
            "Epoch [5/10], Batch [2/3], Loss: 133256.0156\n",
            "Epoch [5/10], Batch [3/3], Loss: 131215.4219\n",
            "Epoch [5/10], Training Loss: 133484.2315\n",
            "Epoch [6/10], Batch [1/3], Loss: 131599.5469\n",
            "Epoch [6/10], Batch [2/3], Loss: 128397.5156\n",
            "Epoch [6/10], Batch [3/3], Loss: 131167.2500\n",
            "Epoch [6/10], Training Loss: 130365.1267\n",
            "Epoch [7/10], Batch [1/3], Loss: 127616.0625\n",
            "Epoch [7/10], Batch [2/3], Loss: 128344.3281\n",
            "Epoch [7/10], Batch [3/3], Loss: 123904.7500\n",
            "Epoch [7/10], Training Loss: 126701.8385\n",
            "Epoch [8/10], Batch [1/3], Loss: 126922.5078\n",
            "Epoch [8/10], Batch [2/3], Loss: 120482.4219\n",
            "Epoch [8/10], Batch [3/3], Loss: 119768.2734\n",
            "Epoch [8/10], Training Loss: 122468.4155\n",
            "Epoch [9/10], Batch [1/3], Loss: 118195.9844\n",
            "Epoch [9/10], Batch [2/3], Loss: 118717.6875\n",
            "Epoch [9/10], Batch [3/3], Loss: 116048.4844\n",
            "Epoch [9/10], Training Loss: 117701.4013\n",
            "Epoch [10/10], Batch [1/3], Loss: 114926.4609\n",
            "Epoch [10/10], Batch [2/3], Loss: 112509.3750\n",
            "Epoch [10/10], Batch [3/3], Loss: 109266.7578\n",
            "Epoch [10/10], Training Loss: 112321.7096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing loop\n",
        "model.eval()\n",
        "test_loss = 0.0\n",
        "with torch.no_grad():\n",
        "    for batch_idx, batch in enumerate(test_loader):\n",
        "        images, landmarks = batch['image'], batch['landmarks']\n",
        "        outputs = model(images)\n",
        "        landmarks = landmarks.view(-1, 8)\n",
        "        loss = criterion(outputs, landmarks.float())\n",
        "        test_loss += loss.item() * images.size(0)\n",
        "        print(f'Test Batch [{batch_idx + 1}/{len(test_loader)}], Loss: {loss.item():.4f}')\n",
        "test_loss /= len(test_dataset)\n",
        "print(f'Test Loss: {test_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeTNNMEOg790",
        "outputId": "40be84e0-4b14-4d71-b17b-1c0bf12d3661"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Batch [1/1], Loss: 104498.4609\n",
            "Test Loss: 104498.4609\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation loop\n",
        "val_loss = 0.0\n",
        "with torch.no_grad():\n",
        "    for batch_idx, batch in enumerate(val_loader):\n",
        "        images, landmarks = batch['image'], batch['landmarks']\n",
        "        outputs = model(images)\n",
        "        landmarks = landmarks.view(-1, 8)\n",
        "        loss = criterion(outputs, landmarks)\n",
        "        val_loss += loss.item() * images.size(0)\n",
        "        print(f'Validation Batch [{batch_idx + 1}/{len(val_loader)}], Loss: {loss.item():.4f}')\n",
        "val_loss /= len(val_dataset)\n",
        "print(f'Validation Loss: {val_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zQsoXbKg9u5",
        "outputId": "06b88324-6718-47a2-b275-caf22d31f380"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Batch [1/1], Loss: 106469.9117\n",
            "Validation Loss: 106469.9117\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference with the trained ResNet model\n",
        "single_image_path = '/content/drive/MyDrive/images/000_HC.png'\n",
        "single_image = Image.open(single_image_path).convert('L')\n",
        "single_image = single_image.resize((256, 256))\n",
        "single_image = transform(single_image).unsqueeze(0)\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    predicted_landmarks = model(single_image)\n",
        "\n",
        "\n",
        "print(\"Predicted Landmarks:\")\n",
        "print(predicted_landmarks)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_14SKO8dhBKF",
        "outputId": "c91fb8ca-3c2b-4450-d2ab-30c65b2cbe90"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Landmarks:\n",
            "tensor([[69.2831, 30.3530, 58.8354, 70.7256, 58.3452, 55.1645, 55.3002, 31.8154]])\n"
          ]
        }
      ]
    }
  ]
}